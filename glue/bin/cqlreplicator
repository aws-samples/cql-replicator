#!/usr/bin/env bash
#
# // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# // SPDX-License-Identifier: Apache-2.0
#
# The following script supports only bash versions 4 and 5
# Migration parameters
declare -A params=(
    [MIGRATOR_VERSION]="1.0"
    [DESCRIPTION]="Migration Toolbox"
    [JOB_NAME]="CQLReplicator"
    [TILES]=4
    [WORKER_TYPE]="G.2X"
    [DISCOVERY_WORKER_TYPE]="G.1X"
    [PROCESS_TYPE_DISCOVERY]="discovery"
    [PROCESS_TYPE_REPLICATION]="replication"
    [PROCESS_TYPE_SAMPLER]="sampler"
    [WRITETIME_COLUMN]="None"
    [TTL_COLUMN]="None"
    [COOLING_PERIOD]=5
    [INCR_TRAFFIC]=240
    [DISCOVERED_TOTAL]=0
    [REPLICATED_TOTAL]=0
    [OVERRIDE_DISCOVERY_WORKERS]=0
    [KEYS_PER_TILE]=0
    [ROWS_PER_WORKER]=1000000
    [TARGET_TYPE]=keyspaces
    [SKIP_GLUE_CONNECTOR]=false
    [SKIP_KEYSPACES_LEDGER]=false
    [REPLICATION_POINT_IN_TIME]=0
    [REPLICATION_STATS_ENABLED]=false
    [GLUE_MONITORING]=false
    [SAFE_MODE]=true
    [SKIP_DISCOVERY]=false
    [MAVEN_REPO]="https://repo1.maven.org/maven2"
    [CLEANUP_REQUESTED]=false
    [REPLAY_LOG]=false
    [MAIN_SCRIPT_LANDING]=false
    [DEFAULT_WORKLOAD_TYPE]=continuous
    [GLUE_TYPE]=gluestreaming
    [WCU_TRAFFIC]=0
    [DEFAULT_ROWS_PER_SECOND]=2000
    [PROCESS_RUNNING]=false
    [TIME_SCHEDULE]=""
)

# Dynamic variables
params[BASE_FOLDER]=$(pwd -L)
params[OS]=$(uname -s)
JSON_MAPPING_B64=$(echo "None" | base64)
JOBS=()
S3_PATH_LIBS=""
DISCOVERY_RUNNING_ID=""
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

# Progress bar configuration
PS=40
PCC="|"
PCU="-"
PPS=2

set +x
# The Art
GREEN='\033[1;32m'
NC='\033[0m' # No Color
cat <(echo -e "${GREEN}") - <(echo -e "${NC}") << "EOF"
    ___ ___  _     ____            _ _           _
  / ___/ _ \| |   |  _ \ ___ _ __ | (_) ___ __ _| |_ ___  _ __
 | |  | | | | |   | |_) / _ \ '_ \| | |/ __/ _` | __/ _ \| '__|
 | |__| |_| | |___|  _ <  __/ |_) | | | (_| (_| | || (_) | |
  \____\__\_\_____|_| \_\___| .__/|_|_|\___\__,_|\__\___/|_|
                            |_|
·······································································
:     __          _______   _____           _____                     :
:    /\ \        / / ____| |  __ \         / ____|                    :
:   /  \ \  /\  / / (___   | |__) | __ ___| (___   ___ _ ____   _____ :
:  / /\ \ \/  \/ / \___ \  |  ___/ '__/ _ \\___ \ / _ \ '__\ \ / / _ \:
: / ____ \  /\  /  ____) | | |   | | | (_) |___) |  __/ |   \ V /  __/:
:/_/    \_\/  \/  |_____/  |_|   |_|  \___/_____/ \___|_|    \_/ \___|:
·······································································
EOF
# Preflight checks
# Check Bash version
if [ "${BASH_VERSINFO[0]}" -lt 4 ]; then
    echo >&2 "Bash version 4 or higher is required. You are using version $BASH_VERSION. Aborting."
    exit 1
fi
command -v aws -v >/dev/null 2>&1 || { echo >&2 "aws cli requires but it's not installed. Aborting."; exit 1; }
command -v curl -V >/dev/null 2>&1 || { echo >&2 "curl requires but it's not installed. Aborting."; exit 1; }
command -v jq -V >/dev/null 2>&1 || { echo >&2 "jq requires but it's not installed. Aborting."; exit 1; }
command -v bc -v >/dev/null 2>&1 || { echo >&2 "bc requires but it's not installed. Aborting. You could try to run: sudo yum install bc -y"; exit 1; }

format_number() {
    local n=$1
    local formatted=""
    local len=${#n}

    for (( i=$len-1; i>=0; i-- )); do
        if [ $(( ($len-$i-1) % 3 )) -eq 0 ] && [ $i -lt $(($len-1)) ]; then
            formatted=",${formatted}"
        fi
        formatted="${n:$i:1}${formatted}"
    done
    echo "$formatted"
}

# Define color codes
LIGHT_GREEN='\033[1;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'

CMD_PARAMS="$@"

log_info() { echo -e "${LIGHT_GREEN}INFO [$(date -Iseconds)]${NC} $*"; }
log_error() { echo -e "${RED}ERROR [$(date -Iseconds)]${NC} $*"; }
log_warn() { echo -e "${YELLOW}WARN [$(date -Iseconds)]${NC} $*"; }

if [[ "${params[OS]}" == "Linux" || "${params[OS]}" == "Darwin" ]]; then
  log_info "OS: ${params[OS]}"
  log_info "AWS CLI: $(aws --version)"
else
  log_error "Error: Please run this script in AWS CloudShell or Linux/Darwin"
  exit 1
fi

function check_input() {
  local input=$1
  local param_name=$2

  if [[ -z $input ]]; then
      log_error "Parameter $param_name empty or null"
      exit 1
  fi
  return 0
}

function max_value() {
  local rs=0
  if [[ $1 -gt $2 ]]; then
    rs="$1"
  else
    rs="$2"
  fi
  echo $rs
}

function confirm() {
  local msg=$1
  read -r -p "$msg" choice
  case $choice in
    y|Y) return 0;;
    n|N) exit 1;;
    *) echo "Invalid choice. Please enter y/Y or n/N." && exit 1;;
  esac
}

print_stat_table() {
  # Assign the arguments to variables
  local tile=$1
  local inserts=$2
  local updates=$3
  local deletes=$4
  local timestamp=$5
  local head=$6
  if [[ $head == true ]]; then
    echo "+------------------------------------------------------------------------+"
    # Print the table header with a border
    printf "| %-8s | %-8s | %-8s | %-8s | %-20s       |\n" "Tile" "Inserts" "Updates" "Deletes" "Timestamp"
    echo "+------------------------------------------------------------------------+"
  fi
  # Print the table data with a border
  printf "| %-8d | %-8d | %-8d | %-8d | %-20s  |\n" $tile $inserts $updates $deletes "$timestamp"
  echo "+------------------------------------------------------------------------+"
}

check_cloudshell() {
    # Method 2: Check for CloudShell user
    if [[ "$(whoami)" == "cloudshell-user" ]]; then
        log_info "Running in AWS CloudShell"
        return 0
    fi

    if [[ -d "/home/cloudshell-user" ]]; then
        log_info "Running in AWS CloudShell"
        return 0
    fi

    log_info "Not running in AWS CloudShell"
    return 1
}

function check_discovery_runs() {
    local rs
    local mode
    # mode = true, if discovery job is not running return 0
    # mode = false, if discovery job is not running return 1
    local mode=$1
    local wait_time=30
    waiting_state=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" \
            --query 'JobRuns[?JobRunState==`WAITING`] | [].Arguments | [?"--PROCESS_TYPE"==`discovery`] | [?"--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?"--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`]' | jq 'length != 0')

    if [[ $waiting_state == true ]]; then
      while [[ $waiting_state == true ]]; do
        sleep $wait_time
        waiting_state=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" \
                    --query 'JobRuns[?JobRunState==`WAITING`] | [].Arguments | [?"--PROCESS_TYPE"==`discovery`] | [?"--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?"--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`]' | jq 'length != 0')
      done
    fi

    rs=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" \
        --query 'JobRuns[?JobRunState==`RUNNING`] | [].Arguments | [?"--PROCESS_TYPE"==`discovery`] | [?"--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?"--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`]' | jq 'length != 0')

    if [[ $rs == "$mode" ]]; then
        log_error "Error: The discovery job ${params[JOB_NAME]}${params[DEFAULT_ENV]} has failed, check AWS Glue logs"
        error_message=$(aws glue get-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --run-id "$DISCOVERY_RUNNING_ID" | jq '.JobRun | {ErrorMessage}')
        log_error "$error_message"
        exit 1
    fi
    return 0
}

function check_target_table_req() {
    local current_mode
    if ! current_mode=$(aws keyspaces get-table --keyspace-name "${params[TARGET_KS]}" --table-name "${params[TARGET_TBL]}" \
        --query 'capacitySpecification' --output json 2>&1); then
        if [[ $current_mode == *"ResourceNotFoundException"* ]]; then
            log_error "Error: Keyspaces or Table ${params[TARGET_KS]}.${params[TARGET_TBL]} not found."
            exit 1
        else
            log_error "Error: Failed to get table information: $current_mode"
            exit 1
        fi
    fi

    if [[ $(jq -r '.throughputMode' <<< "$current_mode") == "PROVISIONED" ]]; then
        local provisioned_to
        provisioned_to=$(jq -r '.writeCapacityUnits' <<< "$current_mode")
        if [[ $provisioned_to -ge ${params[WCU_TRAFFIC]} ]]; then
            log_info "The ${params[TARGET_KS]}.${params[TARGET_TBL]} is provisioned to $provisioned_to WCUs per second"
            log_info "The requested traffic is ${params[WCU_TRAFFIC]}"
        else
            log_warn "Warn: The incoming traffic exceeds the provisioned capacity"
        fi
    else
        log_info "The ${params[TARGET_KS]}.${params[TARGET_TBL]} is on-demand mode"
    fi
}


function check_replication_runs() {
   local tile
   local rs
   tile=$1
   rs=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" \
    --query 'length(JobRuns[?JobRunState==`RUNNING`] | [].Arguments | [?"--PROCESS_TYPE"==`replication`] | [?"--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?"--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`] | [?"--TILE"==`"'"$tile"'"`])')

   if [ "${rs}" -ne 0 ]; then
     log_error "Error: Replication job is already running per tile $tile for ${params[SOURCE_KS]}.${params[SOURCE_TBL]}"
     log_error "$rs"
     return 1
   fi
   return 0
}

function check_num_tiles() {
  if [[ ${params[TILES]} -lt 1 ]]; then
    log_info "Total number of tiles should be => 1"
    exit 1
  fi
  return 0
}

function progress {
  local current="$1"
  local total="$2"
  local title="$3"
  percent=$(bc <<< "scale=$PPS; 100 * $current / $total")
  completed=$(bc <<< "scale=0; $PS * $percent / 100")
  uncompleted=$(bc <<< "scale=0; $PS - $completed")
  completed_sub_bar=$(printf "%${completed}s" | tr " " "${PCC}")
  uncompleted_sub_bar=$(printf "%${uncompleted}s" | tr " " "${PCU}")

  # output the bar
  echo -ne "\r$title" : [${completed_sub_bar}${uncompleted_sub_bar}] ${percent}%

  if [ "$total" -eq "$current" ]; then
      echo -e " - COMPLETED"
  fi
}

function check_file_exists() {
  file=$1
  if [ ! -f "$file" ]; then
    log_error "File $file doesn't exists, please check place files correctly"
    exit 1
  fi
}

function uploader_helper() {
  local artifact_name="$1"
  local curr_pos=$2
  local next_pos=$3
  local final_pos=$4
  check_file_exists "$path_to_conf/$artifact_name"
  progress $curr_pos $final_pos "Uploading $artifact_name                   "
      if ls "$path_to_conf/$artifact_name" > /dev/null
        then
          progress $next_pos $final_pos "Uploading $artifact_name                   "
          aws s3 cp "$path_to_conf"/"$artifact_name" "${params[S3_LANDING_ZONE]}"/artifacts/"$artifact_name" --region "${params[AWS_REGION]}" > /dev/null
        else
          log_error "Error: $path_to_conf/$artifact_name not found"
          exit 1
        fi
}

function uploader_jars() {
  cnt=1
  local artifacts=("$@")
  total_artifacts=$(echo "${artifacts[@]}" | wc -w)
  for link in "${artifacts[@]}"
  do
    file=$(basename "$link")
    progress "$cnt" "$total_artifacts" "Uploading jar artifacts"
    curl -s -O "${params[MAVEN_REPO]}""$link"
    aws s3 cp "$file" "${params[S3_LANDING_ZONE]}"/artifacts/"$file" --region "${params[AWS_REGION]}" > /dev/null
    rm "$file"
    ((cnt++))
  done
}

function join_array() {
    local joined_array=""

    for item in "$@"; do
        joined_array+="${joined_array:+,}$item"
    done
    S3_PATH_LIBS+="$joined_array"
}

function barrier() {
  flag_check_discovery_run="$1"
  local delay=0.1
  local spinstr='|/-\'
  output="Glue job $rs is running"

  while true
  do
    local temp=${spinstr#?}
    printf "\r%s [%c]  " "$output" "$spinstr"
    local spinstr=$temp${spinstr%"$temp"}
    sleep $delay
    cnt=0
    for (( tile=0; tile<"${params[TILES]}"; tile++ ))
    do
      if aws s3 ls "${params[S3_LANDING_ZONE]}"/"${params[SOURCE_KS]}"/"${params[SOURCE_TBL]}"/stats/discovery/"$tile"/ --region "${params[AWS_REGION]}" > /dev/null
      then
        ((cnt++))
      fi
    done
    if [[ $cnt == "${params[TILES]}" ]]; then
      break
    fi
    if [[ $flag_check_discovery_run == "true" ]]; then
      # if the discovery job is not running then fail (return 1)
      sleep 2
      check_discovery_runs "false"
    fi
  done
  printf "\r%s    \n" "$output"
}

function Usage_Exit {
  echo "Usage: $0 [OPTIONS]"
  echo "Script version: ${params[MIGRATOR_VERSION]}"
  echo
  echo "Options:"
  echo "  --state/--cmd STATE/COMMAND      Set the state (init/run/request-stop/stats/cleanup/kill/resize"
  echo "  --tiles, --t NUMBER              Set the number of tiles"
  echo "  --tile, --tl NUMBER              Set the tile number"
  echo "  --worker-type, --wt TYPE         Set the worker type"
  echo "  --discovery-worker-type, --dwt TYPE  Set the discovery worker type"
  echo "  --override-discovery-workers, --odw NUMBER  Override discovery workers"
  echo "  --landing-zone, --lz S3_URI      Set the S3 landing zone"
  echo "  --main-script-landing, --msl     Enable main script landing"
  echo "  --writetime-column, --wc COLUMN  Set the writetime column"
  echo "  --ttl-column, --tc COLUMN        Set the TTL column"
  echo "  --src-keyspace, --sk KEYSPACE    Set the source keyspace"
  echo "  --src-table, --st TABLE          Set the source table"
  echo "  --trg-keyspace, --tk KEYSPACE    Set the target keyspace"
  echo "  --trg-table, --tt TABLE          Set the target table"
  echo "  --inc-traffic, --it              Enable incremental traffic"
  echo "  --custom-inc-traffic, --cit SECONDS  Set custom incremental traffic period"
  echo "  --workload-type, --wlt TYPE      Set the default workload type: batch"
  echo "  --region, --sr REGION            Set the AWS region"
  echo "  --subnet, --ss SUBNET            Set the subnet"
  echo "  --target-subnet, --ts SUBNET     Set the target subnet"
  echo "  --security-groups, --sg GROUPS   Set the security groups"
  echo "  --target-sg, --tsg GROUPS        Set the target security groups"
  echo "  --glue-iam-role, --gir ROLE      Set the Glue IAM role"
  echo "  --availability-zone, --az ZONE   Set the availability zone"
  echo "  --target-az, --taz ZONE          Set the target availability zone"
  echo "  --target-type, --ttp TYPE        Set the target type"
  echo "  --json-mapping, --jm JSON        Set the JSON mapping configuration"
  echo "  --start-replication-from, --srf TIME  Set the replication start time"
  echo "  --override-rows-per-worker, --orw NUMBER  Override rows per worker"
  echo "  --skip-glue-connector, --sgc     Skip Glue connector"
  echo "  --skip-keyspaces-ledger, --skl   Skip Keyspaces ledger"
  echo "  --replication-stats-enabled, --rse  Enable replication stats"
  echo "  --enhanced-monitoring-enabled, --eme  Enable enhanced monitoring"
  echo "  --safe-mode-disabled, --smd      Disable safe mode"
  echo "  --skip-discovery, --sd           Skip discovery job"
  echo "  --cqlreplicator-enviroment, --env ENV  Set CQLReplicator environment"
  echo "  --cleanup-requested, --cr        Request cleanup"
  echo "  --schedule                       Set the cron schedule, for example, 0 * * * *"
  echo
  echo "Commands:"
  echo "  init        Deploy CQLReplicator Glue job and download jars"
  echo "  run         Start migration process"
  echo "  stats       Upload progress (only for historical workload)"
  echo "  request-stop  Stop migration process gracefully"
  echo "  cleanup     Delete all CQLReplicator artifacts"
  echo "  kill        Stop migration process forcefully"
  echo "  resize      Resize the number of tiles"
  echo "  scheduled-run  Schedule CQLReplicator with as a cron job on EC2 instance"
  exit 1
}

check_glue_job_connection() {
    local job_name="$1"
    local aws_region="$2"
    if [ -z "$job_name" ] || [ -z "$aws_region" ]; then
        log_error "Error: Job name and AWS region must be provided."
        return 1
    fi
    job_details=$(aws glue get-job --job-name "$job_name" --region "$aws_region" 2>/dev/null)
    if [ $? -ne 0 ]; then
        log_error "Error: Job $job_name not found in region $aws_region."
        return 1
    fi
    connections=$(echo "$job_details" | jq -r '.Job.Connections.Connections[]' 2>/dev/null)
    if [ -z "$connections" ]; then
        log_warn "Warn: The Glue job $job_name does not have any attached connections."
    fi
}

validate_iam_role_permissions() {
    local role_name="$1"
    local role_arn
    local region="$2"
    local account="$3"
    role_arn=$(aws iam get-role --role-name "$role_name" --query 'Role.Arn' --output text)
    if [ -z "$role_arn" ]; then
        log_error "Error: Unable to find role $role_name"
        return 1
    fi
    local keyspaces_actions=("cassandra:Insert" "cassandra:Update" "cassandra:Select" "cassandra:Delete")
    local s3_actions=("s3:ListBucket" "s3:PutObject" "s3:DeleteObject" "s3:GetObject")

    check_permissions() {
        local service="$1"
        shift
        local actions=("$@")
        local resource_arn
        if [[ $service == "cassandra" ]]; then
          resource_arn="arn:aws:$service:$region:$account:*"
        fi
        if [[ $service == "s3" ]]; then
          bkt_name=$(echo "${params[S3_LANDING_ZONE]}" | sed 's|s3://||')
          resource_arn="arn:aws:$service:$region:$account:$bkt_name"
        fi

        for action in "${actions[@]}"; do
            result=$(aws iam simulate-principal-policy \
                --policy-source-arn "$role_arn" \
                --action-names "$action" \
                --resource-arns $resource_arn \
                --query 'EvaluationResults[0].EvalDecision' \
                --output text)

            if [ "$result" = "allowed" ]; then
                log_info "✅ $action is allowed"
            else
                log_info "❌ $action is not allowed"
            fi
        done
    }

    check_permissions "cassandra" "${keyspaces_actions[@]}"
    check_permissions "s3" "${s3_actions[@]}"
    check_glue_service_role
}

check_glue_service_role() {
    local attached_policies=$(aws iam list-attached-role-policies --role-name "$role_name" --query 'AttachedPolicies[*].PolicyName' --output text)

    if log_info "$attached_policies" | grep -q "AWSGlueServiceRole"; then
        log_info "✅ AWSGlueServiceRole is attached to the role"
    else
        log_info "❌ AWSGlueServiceRole is not attached to the role"

        # Check if the role has a custom policy that might provide Glue permissions
        local inline_policies=$(aws iam list-role-policies --role-name "$role_name" --query 'PolicyNames' --output text)

        if [ -n "$inline_policies" ]; then
            log_info "   Checking inline policies for Glue permissions..."
            for policy in $inline_policies; do
                policy_doc=$(aws iam get-role-policy --role-name "$role_name" --policy-name "$policy" --query 'PolicyDocument' --output json)
                if log_info "$policy_doc" | grep -q "glue:"; then
                    log_info "   ✅ Custom policy '$policy' contains Glue permissions"
                    return
                fi
            done
            log_info "   ❌ No custom policies found with Glue permissions"
        else
            log_info "   ❌ No inline policies found"
        fi
    fi
}

check_security_group_self_referencing() {
    local sg_id="$1"
    sg_rules=$(aws ec2 describe-security-groups \
        --group-ids "$sg_id" \
        --query 'SecurityGroups[0]' \
        --output json)

    inbound_rules=$(echo "$sg_rules" | jq -r '.IpPermissions[] | select(.UserIdGroupPairs[].GroupId == "'"$sg_id"'") | .IpProtocol')

    if [ -n "$inbound_rules" ]; then
        log_info "✅ Self-referencing inbound rule found"
    else
        log_info "❌ No self-referencing inbound rule found"
    fi
}

function Clean_Up {
  check_input "${params[S3_LANDING_ZONE]}" "Error: landing zone parameter is empty, must be provided"
  check_input "${params[AWS_REGION]}" "Error: AWS Region is empty, must be provided"
  aws s3 rm "${params[S3_LANDING_ZONE]}" --recursive --region "${params[AWS_REGION]}"
  aws s3 rb "${params[S3_LANDING_ZONE]}" --region "${params[AWS_REGION]}"
  local connection_name
  connection_name=$(aws glue get-job --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --query 'Job.Connections.Connections[0]' --output text)
  aws glue delete-connection --connection-name "$connection_name" --region "${params[AWS_REGION]}" > /dev/null 2>&1
  aws glue delete-connection --connection-name "cql-replicator-memorydb-integration${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" > /dev/null 2>&1
  aws glue delete-connection --connection-name "cql-replicator-opensearch-integration${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}" > /dev/null 2>&1
  aws glue delete-job --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}"
  if [[ ${params[SKIP_KEYSPACES_LEDGER]} == false ]]; then
    aws keyspaces delete-keyspace --keyspace-name migration --region "${params[AWS_REGION]}"
  fi
}

function Init {
  if [[ ${params[SKIP_GLUE_CONNECTOR]} == false ]]; then
        check_input "${params[AZ]}" "Error: availability zone is, must be provided"
        check_input "${params[SUBNET]}" "Error: subnet is empty, must be provided"
        check_input "${params[SG]}" "Error: sg is empty, must be provided"
    else
        log_info "Skipping glue connector creation"
    fi
    check_input "${params[AWS_REGION]}" "Error: region is empty, must be provided"
    log_info "TARGET TYPE: ${params[TARGET_TYPE]}"

    AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --region "${params[AWS_REGION]}" --output text)
    log_info "Starting initialization process for AWS account: $AWS_ACCOUNT"
    # Create S3 bucket
    if [[ -z ${params[S3_LANDING_ZONE]} ]]; then
        log_info "S3 LANDING ZONE is empty"
        bucket=$(echo "cql-replicator-$AWS_ACCOUNT-${params[AWS_REGION]}${params[DEFAULT_ENV]}" | tr ' [:upper:]' ' [:lower:]')
        params[S3_LANDING_ZONE]="s3://$bucket"
        log_info "Creating a new S3 bucket: ${params[S3_LANDING_ZONE]}"
        if aws s3 mb "${params[S3_LANDING_ZONE]}" > /dev/null 2>&1
        then
          echo "${params[S3_LANDING_ZONE]}" > "working_bucket.dat"
        else
          log_error "Error: not able to create a S3 bucket: ${params[S3_LANDING_ZONE]}"
          exit 1
        fi
    fi

  # Uploading the jars
  ARTIFACTS_BASE=("/com/datastax/spark/spark-cassandra-connector-assembly_2.12/3.4.1/spark-cassandra-connector-assembly_2.12-3.4.1.jar"
  "/software/aws/mcs/aws-sigv4-auth-cassandra-java-driver-plugin/4.0.9/aws-sigv4-auth-cassandra-java-driver-plugin-4.0.9.jar"
  "/com/datastax/oss/java-driver-metrics-micrometer/4.13.0/java-driver-metrics-micrometer-4.13.0.jar"
  "/io/micrometer/micrometer-commons/1.10.4/micrometer-commons-1.10.4.jar"
  "/io/micrometer/micrometer-core/1.10.4/micrometer-core-1.10.4.jar"
  "/io/micrometer/micrometer-observation/1.10.4/micrometer-observation-1.10.4.jar"
  "/io/micrometer/micrometer-registry-cloudwatch/1.10.4/micrometer-registry-cloudwatch-1.10.4.jar")
  ARTIFACTS_KEYSPACES=("/io/vavr/vavr/0.10.4/vavr-0.10.4.jar" "/io/github/resilience4j/resilience4j-retry/1.7.1/resilience4j-retry-1.7.1.jar" "/io/github/resilience4j/resilience4j-core/1.7.1/resilience4j-core-1.7.1.jar")
  ARTIFACTS_MEMORYDB=("/redis/clients/jedis/4.4.6/jedis-4.4.6.jar")
  ARTIFACTS_OSS=("/org/opensearch/client/opensearch-spark-30_2.12/1.0.1/opensearch-spark-30_2.12-1.0.1.jar" "/org/opensearch/driver/opensearch-sql-jdbc/1.4.0.1/opensearch-sql-jdbc-1.4.0.1.jar")
  S3_PATH_BASE=("${params[S3_LANDING_ZONE]}/artifacts/spark-cassandra-connector-assembly_2.12-3.4.1.jar" "${params[S3_LANDING_ZONE]}/artifacts/aws-sigv4-auth-cassandra-java-driver-plugin-4.0.9.jar")
  S3_PATH_KEYSPACES=("${params[S3_LANDING_ZONE]}/artifacts/vavr-0.10.4.jar" "${params[S3_LANDING_ZONE]}/artifacts/resilience4j-retry-1.7.1.jar" "${params[S3_LANDING_ZONE]}/artifacts/resilience4j-core-1.7.1.jar" "${params[S3_LANDING_ZONE]}/artifacts/java-driver-metrics-micrometer-4.13.0.jar" "${params[S3_LANDING_ZONE]}/artifacts/micrometer-commons-1.10.4.jar" "${params[S3_LANDING_ZONE]}/artifacts/micrometer-core-1.10.4.jar" "${params[S3_LANDING_ZONE]}/artifacts/micrometer-observation-1.10.4.jar" "${params[S3_LANDING_ZONE]}/artifacts/micrometer-registry-cloudwatch-1.10.4.jar")
  S3_PATH_MEMORYDB=("${params[S3_LANDING_ZONE]}/artifacts/jedis-4.4.6.jar")
  S3_PATH_OSS=("${params[S3_LANDING_ZONE]}/artifacts/opensearch-spark-30_2.12-1.0.1.jar" "${params[S3_LANDING_ZONE]}/artifacts/opensearch-sql-jdbc-1.4.0.1.jar")

  uploader_jars "${ARTIFACTS_BASE[@]}"
  join_array "${S3_PATH_BASE[@]}"

  if [[ ${params[TARGET_TYPE]} == "keyspaces" ]]; then
    uploader_jars "${ARTIFACTS_KEYSPACES[@]}"
    S3_PATH_LIBS+=","
    join_array "${S3_PATH_KEYSPACES[@]}"
    validate_iam_role_permissions "${params[GLUE_IAM_ROLE]}" "${params[AWS_REGION]}" "$AWS_ACCOUNT"
    security_group_id=$(echo "${params[SG]}" | tr -d "'\"")
    check_security_group_self_referencing "$security_group_id"
  fi

  if [[ ${params[TARGET_TYPE]} == "memorydb" ]]; then
    uploader_jars "${ARTIFACTS_MEMORYDB[@]}"
    S3_PATH_LIBS+=","
    join_array "${S3_PATH_MEMORYDB[@]}"
  fi

  if [[ ${params[TARGET_TYPE]} == "opensearch" ]]; then
    uploader_jars "${ARTIFACTS_OSS[@]}"
    S3_PATH_LIBS+=","
    join_array "${S3_PATH_OSS[@]}"
  fi

  # Uploading the config files
  local path_to_conf
  local path_to_scala
  path_to_conf=$(ls -d "${params[BASE_FOLDER]}" | sed 's/bin/conf/g')
  path_to_scala=$(ls -d "${params[BASE_FOLDER]}" | sed 's/bin/sbin/g')"/${params[TARGET_TYPE]}"

  if [[ ${params[TARGET_TYPE]} == "memorydb" ]]; then
    uploader_helper "RedisConnector.conf" 0 1 5
  fi

  if [[ ${params[TARGET_TYPE]} == "opensearch" ]]; then
    uploader_helper "OpenSearchConnector.conf" 0 1 5
  fi

  # Should be used at least for the ledger
  uploader_helper "KeyspacesConnector.conf" 0 1 5

  # Source C*/K*
  uploader_helper "CassandraConnector.conf" 1 2 5

  local glue_bucket_artifacts
  if [[ "${params[MAIN_SCRIPT_LANDING]}" = false ]]; then
    glue_bucket_artifacts=s3://aws-glue-assets-"$AWS_ACCOUNT"-"${params[AWS_REGION]}"
  else
    glue_bucket_artifacts="${params[S3_LANDING_ZONE]}"
  fi

  if aws s3 ls "$glue_bucket_artifacts"/scripts/ --region "${params[AWS_REGION]}" > /dev/null --region "${params[AWS_REGION]}"
  then
    aws s3 cp "$path_to_scala"/CQLReplicator.scala "$glue_bucket_artifacts"/scripts/${params[JOB_NAME]}${params[DEFAULT_ENV]}.scala --region "${params[AWS_REGION]}" > /dev/null
  else
    aws s3 mb "$glue_bucket_artifacts" --region "${params[AWS_REGION]}" > /dev/null 2>&1
    sleep 25
    if ls "$path_to_scala"/CQLReplicator.scala
    then
      progress 3 5 "Uploading CQLReplicator.scala                  "
      aws s3 cp "$path_to_scala"/CQLReplicator.scala "$glue_bucket_artifacts"/scripts/${params[JOB_NAME]}${params[DEFAULT_ENV]}.scala --region "${params[AWS_REGION]}" > /dev/null
    else
      log_error "Error: $path_to_scala/CQLReplicator.scala not found"
      exit 1
    fi
  fi

  # Create Glue Connector
  local glue_conn_name
  local enhanced_monitoring=""
  local spark_conf="spark.files=${params[S3_LANDING_ZONE]}/artifacts/KeyspacesConnector.conf,${params[S3_LANDING_ZONE]}/artifacts/CassandraConnector.conf --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions --conf spark.kryoserializer.buffer.max=128m --conf spark.rdd.compress=true --conf spark.cleaner.periodicGC.interval=1min --conf spark.kryo.referenceTracking=false --conf spark.cleaner.referenceTracking.cleanCheckpoints=true --conf spark.task.maxFailures=64 --conf spark.shuffle.file.buffer=1mb --conf spark.io.compression.lz4.blockSize=512kb --conf spark.sql.shuffle.partitions=200 --conf spark.default.parallelism=200 --conf spark.locality.wait=0"

  if [[ "${params[GLUE_MONITORING]}" == true ]]; then
      enhanced_monitoring=',"--enable-continuous-cloudwatch-log":"true","--enable-continuous-log-filter":"true","--enable-metrics":"true","--enable-observability-metrics":"true"'
  fi

  if [[ ${params[SKIP_GLUE_CONNECTOR]} == false ]]; then
      progress 3 5 "Creating Glue artifacts                             "
      glue_conn_name=$(echo cql-replicator-"$(uuidgen)" | tr ' [:upper:]' ' [:lower:]')
      aws glue create-connection --connection-input '{
         "Name":"'$glue_conn_name${params[DEFAULT_ENV]}'",
         "Description":"CQLReplicator connection to the C* cluster",
         "ConnectionType":"NETWORK",
         "ConnectionProperties":{
           "JDBC_ENFORCE_SSL": "false"
           },
         "PhysicalConnectionRequirements":{
           "SubnetId":"'${params[SUBNET]}'",
           "SecurityGroupIdList":['${params[SG]}'],
           "AvailabilityZone":"'${params[AZ]}'"}
           }' --region "${params[AWS_REGION]}" --endpoint https://glue."${params[AWS_REGION]}".amazonaws.com --output json

       if [[ ${params[TARGET_TYPE]} == "opensearch" || ${params[TARGET_TYPE]} == "memorydb" ]]; then
         check_input "${params[TRG_SUBNET]}" "Error: subnet for ${params[TARGET_TYPE]} is empty, must be provided"
         check_input "${params[TRG_SG]}" "Error: sg for ${params[TARGET_TYPE]} is empty, must be provided"
         check_input "${params[TRG_AZ]}" "Error: az for ${params[TARGET_TYPE]} is empty, must be provided"
         glue_conn_name_oss="cql-replicator-${params[TARGET_TYPE]}-integration"
         glue_conn_name="$glue_conn_name${params[DEFAULT_ENV]},$glue_conn_name_oss"

         aws glue create-connection --connection-input '{
          "Name":"'$glue_conn_name_oss${params[DEFAULT_ENV]}'",
          "Description":"CQLReplicator connection to '${params[TARGET_TYPE]}'",
          "ConnectionType":"NETWORK",
          "ConnectionProperties":{
            "JDBC_ENFORCE_SSL": "false"
          },
          "PhysicalConnectionRequirements":{
            "SubnetId":"'${params[TRG_SUBNET]}'",
            "SecurityGroupIdList":['${params[TRG_SG]}'],
            "AvailabilityZone":"'${params[TRG_AZ]}'"}
          }' --region "${params[AWS_REGION]}" --endpoint https://glue."${params[AWS_REGION]}".amazonaws.com --output json
       fi

       aws glue create-job \
           --name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" \
           --role "${params[GLUE_IAM_ROLE]}" \
           --description "${params[DESCRIPTION]} for ${params[TARGET_TYPE]}" \
           --glue-version "4.0" \
           --number-of-workers 2 \
           --worker-type "${params[WORKER_TYPE]}" \
           --connections "Connections=$glue_conn_name${params[DEFAULT_ENV]}" \
           --command "Name=${params[GLUE_TYPE]},ScriptLocation=$glue_bucket_artifacts/scripts/${params[JOB_NAME]}${params[DEFAULT_ENV]}.scala" \
           --execution-property '{"MaxConcurrentRuns": 64}' \
           --max-retries 1 \
           --region "${params[AWS_REGION]}" \
           --default-arguments '{
               "--job-language":"scala",
               "--extra-jars":"'$S3_PATH_LIBS'",
               "--conf":"'$spark_conf'",
               "--class":"GlueApp"
               '$enhanced_monitoring'
           }' > /dev/null
   fi

  if [[ ${params[SKIP_GLUE_CONNECTOR]} == true ]]; then
        progress 3 5 "Creating Glue artifacts                             "
        aws glue create-job \
            --name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" \
            --role "${params[GLUE_IAM_ROLE]}" \
            --description "${params[DESCRIPTION]} -> ${params[TARGET_TYPE]}" \
            --glue-version "4.0" \
            --number-of-workers 2 \
            --worker-type "${params[WORKER_TYPE]}" \
            --command "Name=${params[GLUE_TYPE]},ScriptLocation=$glue_bucket_artifacts/scripts/${params[JOB_NAME]}${params[DEFAULT_ENV]}.scala" \
            --execution-property '{"MaxConcurrentRuns": 64}' \
            --max-retries 1 \
            --region "${params[AWS_REGION]}" \
            --default-arguments '{
                "--job-language":"scala",
                "--extra-jars":"'$S3_PATH_LIBS'",
                "--conf":"'$spark_conf'",
                "--class":"GlueApp"
                '${params[enhanced_monitoring]}'
                }' > /dev/null
    fi

  if [[ ${params[SKIP_KEYSPACES_LEDGER]} == true ]]; then
    progress 4 5 "Skipping CQLReplicator's internal keyspace          "
    progress 5 5 "Skipping CQLReplicator's internal table             "
  fi

  if [[ ${params[SKIP_KEYSPACES_LEDGER]} == false ]]; then
    progress 4 5 "Creating CQLReplicator's internal resources         "
    # Create a keyspace - migration
    aws keyspaces create-keyspace --keyspace-name migration --region "${params[AWS_REGION]}" > /dev/null
    sleep 20

    # Create a table - ledger
    aws keyspaces create-table --keyspace-name migration --table-name ledger --schema-definition '{
    "allColumns": [ { "name": "ks", "type": "text" },
    { "name": "tbl", "type": "text" },
    { "name": "tile", "type": "int" },
    { "name": "ver", "type": "text" },
    { "name": "dt_load", "type": "timestamp" },
    { "name": "dt_offload", "type": "timestamp" },
    { "name": "load_status", "type": "text" },
    { "name": "location", "type": "text" },
    { "name": "offload_status", "type": "text" } ],
    "partitionKeys": [ { "name": "ks" }, { "name": "tbl" } ],
    "clusteringKeys": [ { "name": "tile", "orderBy": "ASC" }, { "name": "ver", "orderBy": "ASC" } ] }' --region "${params[AWS_REGION]}" > /dev/null
  progress 5 5 "Created the CQLReplicator internal resources        "
fi

log_info "Deploy is completed"
}

function check_sampler_runs() {
   local process_type="$1"
   local rs
   rs=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" \
      --region "${params[AWS_REGION]}" --query 'JobRuns[?JobRunState==`RUNNING`] | [].Arguments | [?"--PROCESS_TYPE"==`'"$process_type"'`] | [?"--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?"--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`]' | jq 'length != 0')
   params[PROCESS_RUNNING]=$rs
}

function Start_Sampler {
  check_input "${params[SOURCE_KS]}" "Error: source keyspace name is empty, must be provided"
  check_input "${params[SOURCE_TBL]}" "Error: source table name is empty, must be provided"
  check_input "${params[S3_LANDING_ZONE]}" "Error: landing zone must be provided"
  check_input "${params[AWS_REGION]}" "Error: aws region must be provided"
  log_info "SOURCE:" "${params[SOURCE_KS]}"."${params[SOURCE_TBL]}"
  log_info "LANDING ZONE:" "${params[S3_LANDING_ZONE]}"
  check_sampler_runs sampler
  if [ "${params[PROCESS_RUNNING]}" = false ]; then
    rs=$(aws glue start-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" \
          --worker-type "G.025X" --number-of-workers 10 --region "${params[AWS_REGION]}" --arguments '{
          "--PROCESS_TYPE":"'${params[PROCESS_TYPE_SAMPLER]}'",
          "--TILE":"0",
          "--TOTAL_TILES":"1",
          "--S3_LANDING_ZONE":"'${params[S3_LANDING_ZONE]}'",
          "--SOURCE_KS":"'${params[SOURCE_KS]}'",
          "--SOURCE_TBL":"'${params[SOURCE_TBL]}'",
          "--TARGET_KS":"'${params[SOURCE_KS]}'",
          "--TARGET_TBL":"'${params[SOURCE_TBL]}'",
          "--WRITETIME_COLUMN":"'${params[WRITETIME_COLUMN]}'",
          "--SAFE_MODE":"'${params[SAFE_MODE]}'",
          "--REPLICATION_POINT_IN_TIME":"'${params[REPLICATION_POINT_IN_TIME]}'",
          "--CLEANUP_REQUESTED":"'${params[CLEANUP_REQUESTED]}'",
          "--JSON_MAPPING":"'$JSON_MAPPING_B64'",
          "--REPLAY_LOG":"'${params[REPLAY_LOG]}'",
          "--TTL_COLUMN":"'${params[TTL_COLUMN]}'",
          "--WORKLOAD_TYPE":"'${params[DEFAULT_WORKLOAD_TYPE]}'"}' --output text)
    params[PROCESS_RUNNING]=true
    local delay=0.1
    local spinstr='|/-\'
    output="Glue job $rs is running"
    sleep 20
    while [ "${params[PROCESS_RUNNING]}" = true ]; do
      local temp=${spinstr#?}
      printf "\r%s [%c]  " "$output" "$spinstr"
      local spinstr=$temp${spinstr%"$temp"}
      sleep $delay
      check_sampler_runs sampler
    done
    printf "\r%s    \n" "$output"
    sampled_result=$(aws s3 cp "${params[S3_LANDING_ZONE]}"/"${params[SOURCE_KS]}"/"${params[SOURCE_TBL]}"/columnStats/stats.json --region "${params[AWS_REGION]}" -)
    echo "$sampled_result"
  fi
}

function Start_Discovery {
  check_input "${params[SOURCE_KS]}" "Error: source keyspace name is empty, must be provided"
  check_input "${params[SOURCE_TBL]}" "Error: source table name is empty, must be provided"
  check_input "${params[TARGET_KS]}" "Error: target keyspace name is empty, must be provided"
  check_input "${params[TARGET_TBL]}" "Error: target table name is empty, must be provided"
  check_input "${params[S3_LANDING_ZONE]}" "Error: landing zone must be provided"
  check_input "${params[AWS_REGION]}" "Error: aws region must be provided"
  check_target_table_req
  check_num_tiles
  check_glue_job_connection "${params[JOB_NAME]}${params[DEFAULT_ENV]}" "${params[AWS_REGION]}"

  log_info "Your workload is divided into ${params[TILES]} tile(s)"
  log_info "Source: ${params[SOURCE_KS]}.${params[SOURCE_TBL]}"
  log_info "Target: ${params[TARGET_KS]}.${params[TARGET_TBL]}"
  log_info "Landing zone for artifacts and temp files: ${params[S3_LANDING_ZONE]}"
  log_info "Column used to detect changes in the source: ${params[WRITETIME_COLUMN]}"
  log_info "TTL column: ${params[TTL_COLUMN]}"
  log_info "Each Glue DPU will handle up to ${params[ROWS_PER_WORKER]} rows (will be ignored if you use --max-wcu-traffic)"
  log_info "Replication start timestamp: ${params[REPLICATION_POINT_IN_TIME]} (0 means disabled)"
  log_info "Safe mode: ${params[SAFE_MODE]} (reduces overhead on the source)"
  log_info "Default worker type: ${params[WORKER_TYPE]}"

  local workers=0
  if [[ ${params[OVERRIDE_DISCOVERY_WORKERS]} == 0 ]]; then
    max_value2=2
    max_value1=$((2 * ${params[TILES]} + 1))
    workers=$(max_value $max_value1 $max_value2)
  else
    workers=${params[OVERRIDE_DISCOVERY_WORKERS]}
  fi
  # TCO
  total_dpu=$(( workers + ( params[DEFAULT_WORKERS] * params[TILES] )))
  "$SCRIPT_DIR"/helper --state get-tco --total-dpu $total_dpu --region ${params[AWS_REGION]}
  log_info "Checking if the discovery job is already running..."
  if [[ ${params[CLEANUP_REQUESTED]} == "true" ]]; then
    log_info "The ledger is going to be cleaned up"
    confirm "<=== Do you want to continue? ===> "
    log_info "Deleting objects in ${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}..."
    aws s3 rm "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}" --recursive --region "${params[AWS_REGION]}" > /dev/null 2>&1
  fi
  if check_discovery_runs "true"; then
    Delete_Stop_Event_D
    log_info "Starting the discovery job..."
    rs=$(aws glue start-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --worker-type "${params[DISCOVERY_WORKER_TYPE]}" --number-of-workers "$workers" --region "${params[AWS_REGION]}" --arguments '{"--PROCESS_TYPE":"'${params[PROCESS_TYPE_DISCOVERY]}'",
        "--TILE":"0",
        "--TOTAL_TILES":"'${params[TILES]}'",
        "--S3_LANDING_ZONE":"'${params[S3_LANDING_ZONE]}'",
        "--SOURCE_KS":"'${params[SOURCE_KS]}'",
        "--SOURCE_TBL":"'${params[SOURCE_TBL]}'",
        "--TARGET_KS":"'${params[TARGET_KS]}'",
        "--TARGET_TBL":"'${params[TARGET_TBL]}'",
        "--WRITETIME_COLUMN":"'${params[WRITETIME_COLUMN]}'",
        "--SAFE_MODE":"'${params[SAFE_MODE]}'",
        "--REPLICATION_POINT_IN_TIME":"'${params[REPLICATION_POINT_IN_TIME]}'",
        "--CLEANUP_REQUESTED":"'${params[CLEANUP_REQUESTED]}'",
        "--JSON_MAPPING":"'$JSON_MAPPING_B64'",
        "--REPLAY_LOG":"'${params[REPLAY_LOG]}'",
        "--TTL_COLUMN":"'${params[TTL_COLUMN]}'",
        "--WORKLOAD_TYPE":"'${params[DEFAULT_WORKLOAD_TYPE]}'"}' --output text)
    DISCOVERY_RUNNING_ID=$rs
    JOBS+=("$rs")
  fi
}

function Resize_Cluster {
  check_input "${params[SOURCE_KS]}" "Error: source keyspace name is empty, must be provided"
    check_input "${params[SOURCE_TBL]}" "Error: source table name is empty, must be provided"
    check_input "${params[S3_LANDING_ZONE]}" "Error: landing zone must be provided"
    check_input "${params[AWS_REGION]}" "Error: aws region must be provided"
    check_input "${params[TILES]}" "Error: new number of tiles should be provided"
    log_info "SOURCE:" "${params[SOURCE_KS]}"."${params[SOURCE_TBL]}"
    log_info "LANDING ZONE:" "${params[S3_LANDING_ZONE]}"
    log_info "WORKER_TYPE: ${params[WORKER_TYPE]}"
    log_info "Resizing the tiles. The new size is ${params[TILES]}"
    check_sampler_runs resize
    if [ "${params[PROCESS_RUNNING]}" = false ]; then
      aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/primaryKeys" \
                    "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/primaryKeysCopy" \
                    --recursive --region "${params[AWS_REGION]}" >/dev/null
      # Check if primaryKeysCopy exists if the prefix doesn't exists return error and exit
      if aws s3 ls "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/primaryKeysCopy/" --region "${params[AWS_REGION]}" > /dev/null; then
        log_info "The copy has been created"
      else
        log_error "No copy found in ${params[S3_LANDING_ZONE]}"
        exit 1
      fi
      log_info "Removing the primary keys and stats"
      aws s3 rm "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/primaryKeys" \
                    --recursive --region "${params[AWS_REGION]}" >/dev/null
      aws s3 rm "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats" \
                    --recursive --region "${params[AWS_REGION]}" >/dev/null
      rs=$(aws glue start-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --worker-type "G.1X" --number-of-workers $(( ${params[TILES]} + 1 )) --region "${params[AWS_REGION]}" --arguments '{
            "--PROCESS_TYPE":"resize",
            "--TILE":"0",
            "--TOTAL_TILES":"'${params[TILES]}'",
            "--S3_LANDING_ZONE":"'${params[S3_LANDING_ZONE]}'",
            "--SOURCE_KS":"'${params[SOURCE_KS]}'",
            "--SOURCE_TBL":"'${params[SOURCE_TBL]}'",
            "--TARGET_KS":"'${params[SOURCE_KS]}'",
            "--TARGET_TBL":"'${params[SOURCE_TBL]}'",
            "--WRITETIME_COLUMN":"'${params[WRITETIME_COLUMN]}'",
            "--SAFE_MODE":"'${params[SAFE_MODE]}'",
            "--REPLICATION_POINT_IN_TIME":"'${params[REPLICATION_POINT_IN_TIME]}'",
            "--CLEANUP_REQUESTED":"'${params[CLEANUP_REQUESTED]}'",
            "--JSON_MAPPING":"'$JSON_MAPPING_B64'",
            "--REPLAY_LOG":"'${params[REPLAY_LOG]}'",
            "--TTL_COLUMN":"'${params[TTL_COLUMN]}'",
            "--WORKLOAD_TYPE":"'${params[DEFAULT_WORKLOAD_TYPE]}'"}' --output text)
      params[PROCESS_RUNNING]=true
      local delay=0.1
      local spinstr='|/-\'
      output="Glue job $rs is running. Resizing in progress..."
      sleep 20
      while [ "${params[PROCESS_RUNNING]}" = true ]; do
        local temp=${spinstr#?}
        printf "\r%s [%c]  " "$output" "$spinstr"
        local spinstr=$temp${spinstr%"$temp"}
        sleep $delay
        check_sampler_runs resize
      done
      printf "\r%s    \n" "$output"
      log_info "Removing copy of the primary keys"
      aws s3 rm "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/primaryKeysCopy" \
                    --recursive --region "${params[AWS_REGION]}" >/dev/null
    fi
}

function Kill_Replication {
    local process_type="$1"

    rs=$(aws glue get-job-runs --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --region "${params[AWS_REGION]}"  \
        --query 'JobRuns[?JobRunState==`RUNNING`] | [].{Id: Id, Arguments: Arguments} | [?Arguments."--PROCESS_TYPE"==`'"${process_type}"'`] | [?Arguments."--SOURCE_KS"==`'"${params[SOURCE_KS]}"'`] | [?Arguments."--SOURCE_TBL"==`'"${params[SOURCE_TBL]}"'`]' |  jq -r '.[].Id')

    for job_id in $rs; do
      aws glue batch-stop-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --job-run-ids "$job_id" --region "${params[AWS_REGION]}"
    done
}

function Start_Replication {
  cnt=0
  KEYS_PER_TILE=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/discovery/$cnt/count.json" --region "${params[AWS_REGION]}" - | head | jq '.primaryKeys')
  log_info "Sampled primary keys per tile is $KEYS_PER_TILE"
  local workers=$(( 2 + KEYS_PER_TILE/${params[ROWS_PER_WORKER]} ))
  if [[ workers -gt 299 ]]; then
    log_info "=== The total number of Glue workers are over 299 per each tile (glue job), you have two options:                  ==="
    log_info "=== [1] Increase the number of tiles. In order to rerun with the higher number of tiles                            ==="
    log_info "=== request a stop for this migration process, and after rerun again with the flag --cleanup-requested             ==="
    log_info "======================================================================================================================"
    log_info "=== [2] Increase the number of primary keys per worker. Default value is ${params[ROWS_PER_WORKER]} per worker                         ==="
    log_info "=== request a stop for this migration process and rerun again with the flag --override-rows-per-worker <new value> ==="
    exit 1
  fi
  while [ $cnt -lt ${params[TILES]} ]
  do
    if check_replication_runs $cnt; then
      Delete_Stop_Event_R $cnt
      if [ "${params[WCU_TRAFFIC]}" -gt 0 ]; then
          workers=${params[DEFAULT_WORKERS]}
      fi
      rs=$(aws glue start-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --worker-type "${params[WORKER_TYPE]}" --number-of-workers "$workers" --region "${params[AWS_REGION]}" --arguments '{"--PROCESS_TYPE":"'${params[PROCESS_TYPE_REPLICATION]}'",
          "--TILE":"'$cnt'",
          "--TOTAL_TILES":"'${params[TILES]}'",
          "--S3_LANDING_ZONE":"'${params[S3_LANDING_ZONE]}'",
          "--SOURCE_KS":"'${params[SOURCE_KS]}'",
          "--SOURCE_TBL":"'${params[SOURCE_TBL]}'",
          "--TARGET_KS":"'${params[TARGET_KS]}'",
          "--TARGET_TBL":"'${params[TARGET_TBL]}'",
          "--WRITETIME_COLUMN":"'${params[WRITETIME_COLUMN]}'",
          "--SAFE_MODE":"'${params[SAFE_MODE]}'",
          "--REPLICATION_POINT_IN_TIME":"'${params[REPLICATION_POINT_IN_TIME]}'",
          "--CLEANUP_REQUESTED":"false",
          "--JSON_MAPPING":"'$JSON_MAPPING_B64'",
          "--REPLAY_LOG":"'${params[REPLAY_LOG]}'",
          "--TTL_COLUMN":"'${params[TTL_COLUMN]}'",
          "--WORKLOAD_TYPE":"'${params[DEFAULT_WORKLOAD_TYPE]}'"}' --output text)
       JOBS+=("$rs")
      sleep ${params[COOLING_PERIOD]}
    fi
    ((cnt++))
    progress "$cnt" "${params[TILES]}" "Starting Glue Jobs"
  done
}

function Start_Single_Replication {
  cnt=${params[TILE]}
  KEYS_PER_TILE=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/discovery/$cnt/count.json" --region "${params[AWS_REGION]}" - | head | jq '.primaryKeys')
  log_info "Sampled primary keys per tile is $KEYS_PER_TILE"
  local workers=$(( 2 + KEYS_PER_TILE/${params[ROWS_PER_WORKER]} ))
  if [[ workers -gt 299 ]]; then
    log_info "=== The total number of Glue workers are over 299 per each tile (glue job), you have two options:                  ==="
    log_info "=== [1] Increase the number of tiles. In order to rerun with the higher number of tiles                            ==="
    log_info "=== request a stop for this migration process, and after rerun again with the flag --cleanup-requested             ==="
    log_info "======================================================================================================================"
    log_info "=== [2] Increase the number of primary keys per worker. Default value is ${params[ROWS_PER_WORKER]} per worker                         ==="
    log_info "=== request a stop for this migration process and rerun again with the flag --override-rows-per-worker <new value> ==="
    exit 1
  fi
  if check_replication_runs "$cnt"; then
    Delete_Stop_Event_R "$cnt"
    if [ "${params[WCU_TRAFFIC]}" -gt 0 ]; then
        workers=${params[DEFAULT_WORKERS]}
    fi
    rs=$(aws glue start-job-run --job-name "${params[JOB_NAME]}${params[DEFAULT_ENV]}" --worker-type "${params[WORKER_TYPE]}" --number-of-workers "$workers" --region "${params[AWS_REGION]}" --arguments '{"--PROCESS_TYPE":"'${params[PROCESS_TYPE_REPLICATION]}'",
          "--TILE":"'$cnt'",
          "--TOTAL_TILES":"'${params[TILES]}'",
          "--S3_LANDING_ZONE":"'${params[S3_LANDING_ZONE]}'",
          "--SOURCE_KS":"'${params[SOURCE_KS]}'",
          "--SOURCE_TBL":"'${params[SOURCE_TBL]}'",
          "--TARGET_KS":"'${params[TARGET_KS]}'",
          "--TARGET_TBL":"'${params[TARGET_TBL]}'",
          "--WRITETIME_COLUMN":"'${params[WRITETIME_COLUMN]}'",
          "--SAFE_MODE":"'${params[SAFE_MODE]}'",
          "--REPLICATION_POINT_IN_TIME":"'${params[REPLICATION_POINT_IN_TIME]}'",
          "--CLEANUP_REQUESTED":"false",
          "--JSON_MAPPING":"'$JSON_MAPPING_B64'",
          "--REPLAY_LOG":"'${params[REPLAY_LOG]}'",
          "--TTL_COLUMN":"'${params[TTL_COLUMN]}'",
          "--WORKLOAD_TYPE":"'${params[DEFAULT_WORKLOAD_TYPE]}'"}' --output text)
    JOBS+=("$rs")
  fi
}

function validate_json() {
  local json_str=$1

  # Check if the JSON is valid
  echo "$json_str" | jq empty
  if [[ $? -ne 0 ]]; then
      log_error "Error: Invalid JSON"
      log_error '{"column": "column_name", "bucket": "bucket-name", "prefix": "keyspace_name/table_name/payload", "xref": "reference-column"}'
      exit 1
  fi

  # Check for empty values
  empty_values=$(echo "$json_str" | jq 'recurse | select(. == "" or . == null)')
  if [[ -n $empty_values ]]; then
      log_error "Error: JSON contains empty values"
      return 1
  fi

  # Check if proper keys exist
  local column
  local bucket
  local prefix
  local xref
  column=$(echo "$json_str" | jq -r '.column')
  bucket=$(echo "$json_str" | jq -r '.bucket')
  prefix=$(echo "$json_str" | jq -r '.prefix')
  xref=$(echo "$json_str" | jq -r '.xref')

  if [[ "$column" == null || "$bucket" == null || "$prefix" == null || "$xref" == null ]]; then
      log_error "Error: JSON doesn't contain required keys: column, bucket, xref, and prefix"
      return 1
  fi

  return 0
}

function Delete_Stop_Event_D {
  aws s3api delete-object --bucket "${params[S3_LANDING_ZONE]:5}" --key "${params[SOURCE_KS]}/${params[SOURCE_TBL]}/discovery/stopRequested" --region "${params[AWS_REGION]}"
}

function Delete_Stop_Event_R {
  aws s3api delete-object --bucket "${params[S3_LANDING_ZONE]:5}" --key "${params[SOURCE_KS]}/${params[SOURCE_TBL]}/replication/$1/stopRequested" --region "${params[AWS_REGION]}"
}

function Request_Stop {
  tile=0
  log_info "Requested a stop for the discovery job"

  if aws s3api put-object --bucket "${params[S3_LANDING_ZONE]:5}" --key "${params[SOURCE_KS]}/${params[SOURCE_TBL]}/discovery/stopRequested" --region "${params[AWS_REGION]}" >/dev/null
  then
    while [ $tile -lt ${params[TILES]} ]
    do
      log_info "Requested a stop for the replication tile: $tile"
      aws s3api put-object --bucket "${params[S3_LANDING_ZONE]:5}" --key "${params[SOURCE_KS]}/${params[SOURCE_TBL]}/replication/$tile/stopRequested" --region "${params[AWS_REGION]}" >/dev/null
      ((tile++))
    done
  fi
}

function Request_Single_Stop {
  tile=${params[TILE]}
  log_info "Requested a stop for the replication tile: $tile"
  aws s3api put-object --bucket "${params[S3_LANDING_ZONE]:5}" --key "${params[SOURCE_KS]}/${params[SOURCE_TBL]}/replication/$tile/stopRequested" --region "${params[AWS_REGION]}" >/dev/null
}

calculate_resources() {
  local workers=0
  local drps=${params[DEFAULT_ROWS_PER_SECOND]}
  local tiles=${params[TILES]}
  local wcu=${params[WCU_TRAFFIC]}
  if [ -z "${params[WCU_TRAFFIC]}" ]; then
    log_error "Error: WCU_TRAFFIC is not set"
    return 1
  fi

  if [ "${params[WCU_TRAFFIC]}" -le 10000 ]; then
    k=-1
    params[WORKER_TYPE]="G.025X"
    f=$(( drps * tiles ))
    workers=$(( (wcu + f - 1) / f - k ))
  elif [ "${params[WCU_TRAFFIC]}" -ge 10000 ]; then
    k=1
    params[WORKER_TYPE]="G.1X"
    f=$(( drps * tiles ))
    workers=$(( (wcu + f - 1) / f - k ))
  fi
  params[DEFAULT_WORKERS]=$workers
}

(( $#<1 )) && Usage_Exit

while (( "$#" )); do
  case "$1" in
    --state|--cmd)
      params[STATE]="$2"
      shift 2
      ;;
    --tiles|--t)
      params[TILES]="$2"
      shift 2
      ;;
    --tile|--tl)
      params[TILE]="$2"
      shift 2
      ;;
    --worker-type|--wt)
      params[WORKER_TYPE]="$2"
      shift 2
      ;;
    --discovery-worker-type|--dwt)
      params[DISCOVERY_WORKER_TYPE]="$2"
      shift 2
      ;;
    --override-discovery-workers|--odw)
      params[OVERRIDE_DISCOVERY_WORKERS]="$2"
      shift 2
      ;;
    --landing-zone|--lz)
      params[S3_LANDING_ZONE]="$2"
      shift 2
      ;;
    --main-script-landing|--msl)
      params[MAIN_SCRIPT_LANDING]="true"
      shift 1
      ;;
    --writetime-column|--wc)
      params[WRITETIME_COLUMN]="$2"
      shift 2
      ;;
    --ttl-column|--tc)
      params[TTL_COLUMN]="$2"
      shift 2
      ;;
    --src-keyspace|--sk)
      params[SOURCE_KS]="$2"
      shift 2
      ;;
    --src-table|--st)
      params[SOURCE_TBL]="$2"
      shift 2
      ;;
    --trg-keyspace|--tk)
      params[TARGET_KS]="$2"
      shift 2
      ;;
    --trg-table|--tt)
      params[TARGET_TBL]="$2"
      shift 2
      ;;
    --inc-traffic|--it)
      #You can increase this value to reduce traffic pressure
      params[COOLING_PERIOD]="${params[INCR_TRAFFIC]}"
      log_info "Incremental traffic for the historical workload is enabled"
      log_info "Incremental period: ${params[COOLING_PERIOD]} seconds"
      shift 1
      ;;
    --custom-inc-traffic|--cit)
      #You can increase this value to reduce traffic pressure
      params[INCR_TRAFFIC]="$2"
      params[COOLING_PERIOD]="$2"
      log_info "Custom incremental traffic for the historical workload is enabled"
      log_info "Custom incremental period: ${params[COOLING_PERIOD]} seconds"
      shift 2
      ;;
    --workload-type|--wlt)
      params[DEFAULT_WORKLOAD_TYPE]="$2"
      log_info "Default workload type: ${params[DEFAULT_WORKLOAD_TYPE]}"
      shift 2
      ;;
    --region|--sr)
      params[AWS_REGION]="$2"
      shift 2
      ;;
    --subnet|--ss)
      params[SUBNET]="$2"
      shift 2
      ;;
    --target-subnet|--ts)
      params[TRG_SUBNET]="$2"
      shift 2
      ;;
    --security-groups|--sg)
      params[SG]="$2"
      shift 2
      ;;
    --target-sg|--tsg)
      params[TRG_SG]="$2"
      shift 2
      ;;
    --glue-iam-role|--gir)
      params[GLUE_IAM_ROLE]="$2"
      shift 2
      ;;
    --availability-zone|--az)
      params[AZ]="$2"
      shift 2
      ;;
    --target-az|--taz)
      params[TRG_AZ]="$2"
      shift 2
      ;;
    --target-type|--ttp)
      params[TARGET_TYPE]="$2"
      shift 2
      ;;
    --json-mapping|--jm)
      JSON_MAPPING="$2"
      echo "$JSON_MAPPING" | jq empty
      log_info "Provided a json mapping configuration $JSON_MAPPING"
      # Should work for AWS CloudShell, use 0 to disable line wrapping
      if [[ ${params[OS]} == Linux ]]; then
        JSON_MAPPING_B64=$(echo "$JSON_MAPPING" | base64 -w 0)
      fi
      # Use base64 without params for MacOS instead
      if [[ ${params[OS]} == Darwin ]]; then
        JSON_MAPPING_B64=$(echo "$JSON_MAPPING" | base64)
      fi
      shift 2
      ;;
    --start-replication-from|--srf)
      params[REPLICATION_POINT_IN_TIME]="$2"
      shift 2
      ;;
    --override-rows-per-worker|--orw)
      params[ROWS_PER_WORKER]="$2"
      shift 2
      ;;
    --skip-glue-connector|--sgc)
      params[SKIP_GLUE_CONNECTOR]=true
      shift 1
      ;;
    --skip-keyspaces-ledger|--skl)
      params[SKIP_KEYSPACES_LEDGER]=true
      shift 1
      ;;
    --replication-stats-enabled|--rse)
      params[REPLICATION_STATS_ENABLED]=true
      shift 1
      ;;
    --enhanced-monitoring-enabled|--eme)
      params[GLUE_MONITORING]=true
      shift 1
      ;;
    --safe-mode-disabled|--smd)
      params[SAFE_MODE]=false
      shift 1
      ;;
    --skip-discovery|--sd)
      params[SKIP_DISCOVERY]="true"
      log_info "Skipping the discovery job"
      shift 1
      ;;
    --cqlreplicator-enviroment|--env)
      if [[ "$2" != "" ]]; then
          log_info "Environment name: $2"
          params[DEFAULT_ENV]="-$2"
      fi
      shift 2
      ;;
    --cleanup-requested|--cr)
      log_info "The ledger is going to be cleaned up"
      params[CLEANUP_REQUESTED]="true"
      shift 1
      ;;
    --replay-log|--rl)
      log_info "The log of failed retries will be replayed, in order: inserts, updates, and deletes"
      params[REPLAY_LOG]=true
      shift 1
      ;;
    --max-wcu-traffic|--mwt)
      params[WCU_TRAFFIC]="$2"
      calculate_resources
      log_info "The write traffic set up to ${params[WCU_TRAFFIC]} WCUs per second"
      log_info "The workers per tile is ${params[DEFAULT_WORKERS]}"
      shift 2
      ;;
     --schedule)
       params[TIME_SCHEDULE]="$2"
       shift 2
       ;;
    --)
      shift
      break
      ;;
    -*|--*=)
      Usage_Exit
      ;;
    *)
      PARAMS="$PARAMS $1"
      shift
      ;;
  esac
done

eval set -- "$PARAMS"

if [[ ${params[STATE]} == run ]]; then
  if [[ ${params[CLEANUP_REQUESTED]} != "true" ]]; then
    "$SCRIPT_DIR"/helper --state recovery --tiles "${params[TILES]}" \
    --landing-zone "${params[S3_LANDING_ZONE]}" --region "${params[AWS_REGION]}" \
    --src-keyspace "${params[SOURCE_KS]}" --src-table "${params[SOURCE_TBL]}"
    RETURN_CODE=$?
    if [  $RETURN_CODE -ne 0 ]; then
        log_error "Recovery step has failed, the code is ${RETURN_CODE}"
        exit $RETURN_CODE
    fi
  fi

  if [[ ${params[SKIP_DISCOVERY]} == "false" ]]; then
    Start_Discovery
    barrier "true"
  fi
  Start_Replication
  log_info "Started jobs:" "${JOBS[@]}"
fi

if [[ ${params[STATE]} == sample ]]; then
  Start_Sampler
fi

if [[ ${params[STATE]} == resize ]]; then
  Resize_Cluster
fi

if [[ ${params[STATE]} == run-single ]]; then
  check_input "${params[TILE]}" "Error: tile parameter is empty, must be provided"
  Start_Single_Replication
  log_info "Started jobs:" "${JOBS[@]}"
fi

if [[ ${params[STATE]} == request-single-stop ]]; then
  check_input "${params[TILE]}" "Error: tile parameter is empty, must be provided"
  Request_Single_Stop
fi

if [[ ${params[STATE]} == request-stop ]]; then
  Request_Stop
fi

if [[ ${params[STATE]} == kill ]]; then
  Kill_Replication discovery
  Kill_Replication replication
fi

if [[ ${params[STATE]} == scheduled-run ]]; then
  if check_cloudshell; then
    log_error "You can't run cron in CloudShell, please use an EC2 instance"
    exit 1
  else
    log_info "Scheduling CQLReplicator..."
    CMD_PARAMS="${CMD_PARAMS% --cr}"
    CMD_PARAMS="${CMD_PARAMS/scheduled-run/run}"
    if [[ "${params[WORKLOAD_TYPE]}" = "" ]]; then
      CMD_PARAMS="$CMD_PARAMS --workload-type batch"
    fi
    cqlreplicator_home="$(pwd)"
    cqlreplicator_bin="/bin/bash $cqlreplicator_home/cqlreplicator"
    cmd_final="$cqlreplicator_bin $CMD_PARAMS >> $cqlreplicator_home/cron.log 2>&1"
    # Examples with different minutes:
    # 0 * * * *     # Runs at minute 0 of every hour (1:00, 2:00, 3:00, etc.)
    # 30 * * * *    # Runs at minute 30 of every hour (1:30, 2:30, 3:30, etc.)
    # 15 * * * *    # Runs at minute 15 of every hour (1:15, 2:15, 3:15, etc.)
    # 0/15 * * * *  # Runs every 15 minutes
    # eval "$(crontab -l | sed -n '1p' | tr -s ' ' | cut -d' ' -f 6-)"
    log_info "The cron schedule: ${params[TIME_SCHEDULE]}"
    if (crontab -l 2>/dev/null; echo "${params[TIME_SCHEDULE]} $cmd_final") | crontab -; then
              log_info "Successfully scheduled cron job"
          else
              log_error "Failed to schedule cron job"
              exit 1
          fi
    exit 0
  fi
fi

function Gather_Stats() {
   tile=$1
   process_type=$2
   local total_per_tile=0
   if aws s3 ls "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/" --region "${params[AWS_REGION]}" > /dev/null
   then
     if [[ $process_type == "${params[PROCESS_TYPE_DISCOVERY]}" ]]; then
       total_per_tile=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.primaryKeys') && params[DISCOVERED_TOTAL]=$(( ${params[DISCOVERED_TOTAL]} + total_per_tile ))
     fi
     if [[ $process_type == "${params[PROCESS_TYPE_REPLICATION]}" ]]; then
       if  aws s3 ls "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/" --region "${params[AWS_REGION]}" > /dev/null
         then
         total_per_tile=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.primaryKeys') && params[REPLICATED_TOTAL]=$(( ${params[REPLICATED_TOTAL]} + total_per_tile ))
      fi
      if [[ ${params[REPLICATION_STATS_ENABLED]} == true ]]; then
        if aws s3 ls "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile" --region "${params[AWS_REGION]}" > /dev/null
        then
          local inserted=0
          inserted=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.insertedPrimaryKeys')
          local updated=0
          updated=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.updatedPrimaryKeys')
          local deleted=0
          deleted=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.deletedPrimaryKeys')
          local timestamp=""
          timestamp=$(aws s3 cp "${params[S3_LANDING_ZONE]}/${params[SOURCE_KS]}/${params[SOURCE_TBL]}/stats/$process_type/$tile/count.json" --region "${params[AWS_REGION]}" - | head | jq '.updatedTimestamp')
          local header=true
          if [[ $tile != 0 ]]; then
            header=false
          fi
          print_stat_table "$tile" "$inserted" "$updated" "$deleted" "$timestamp" "$header"
        fi
      fi
    fi
  fi
}

if [[ ${params[STATE]} == init ]]; then
 Init
fi

if [[ ${params[STATE]} == cleanup ]]; then
  log_info "Deleting deployed artifacts: the glue connection (optional), the S3 bucket, and the glue job"
  Clean_Up
fi

if [[ ${params[STATE]} == stats ]]; then
  check_input "${params[SOURCE_KS]}" "Error: source keyspace name is empty, must be provided"
  check_input "${params[SOURCE_TBL]}" "Error: source table name is empty, must be provided"
  check_input "${params[S3_LANDING_ZONE]}" "Error: landing zone must be provided"
  check_input "${params[AWS_REGION]}" "Error: aws region must be provided"
  check_input "${params[SOURCE_KS]}" "Error: source keyspace name is empty, must be provided"
  check_input "${params[SOURCE_TBL]}" "Error: source table name is empty, must be provided"
  check_input "${params[TARGET_TBL]}" "Error: target table name is empty, must be provided"
  check_input "${params[TARGET_KS]}" "Error: target keyspace name is empty, must be provided"
  tile=0
  while [ $tile -lt "${params[TILES]}" ]
    do
      Gather_Stats $tile "${params[PROCESS_TYPE_DISCOVERY]}"
      Gather_Stats $tile "${params[PROCESS_TYPE_REPLICATION]}"
      ((tile++))
    done
  log_info "Discovered rows in" "${params[SOURCE_KS]}"."${params[SOURCE_TBL]}" is "$(format_number ${params[DISCOVERED_TOTAL]})"
  log_info "Replicated rows in" "${params[TARGET_KS]}"."${params[TARGET_TBL]}" is "$(format_number ${params[REPLICATED_TOTAL]})"
  if [[ ${params[REPLICATION_STATS_ENABLED]} == true ]]; then
    t=0
    while [ $t -lt "${params[TILES]}" ]
    do
      Gather_Stats $t "detailed-replication"
      ((t++))
    done
  fi
fi